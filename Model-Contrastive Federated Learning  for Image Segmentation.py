# -*- coding: utf-8 -*-
"""FL_CL_TF_v2_shashank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Adt2jrim1_Ngru9EYNyhsL9qKBHGnxfM
"""

# import psutil
# split_bar = '='*20
# memory_info = psutil.virtual_memory()._asdict()
# print(f"{split_bar} Memory Usage {split_bar}")
# for k,v in memory_info.items():
#   print(k, v)
# print(f"{split_bar} CPU Usage {split_bar}")
# print(f"CPU percent: {psutil.cpu_percent()}%")

# from google.colab import drive
# drive.mount('/content/drive')

'''

!pip install tensorflow_addons
!pip install nibabel
#!pip install tensorflow==2.8
#!apt install --all ow-change-held-packages libcudnn8 = 8.1.0.77-1+cuda11.2
!pip install segmentation_models_3D

'''


import nibabel as nib
import cv2
import gc
import tensorflow_addons as tfa
import os
import numpy as np
from glob import glob
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}
import tensorflow as tf
from tqdm.auto import tqdm
import keras.backend as K
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import segmentation_models_3D as sm
import pandas as pd

print(tf.__version__)

"""### Preprocessing Datasets """

#preprocessing
class preprocess:
    def __init__(self,dir_path,dataset = "MSD"):
        self.path = dir_path
        self.dataset = dataset
    
    #clean files
    def clean_files(self, filenames):
        file_to_remove = []
        for file in filenames:
            if file.startswith("._BRATS"):
                file_to_remove.append(file)
        for file in file_to_remove:
            filenames.remove(file)
        return filenames

    def get_path(self):
        bratfiles = os.listdir(self.path)
        if self.dataset == "brats20":
            print("Preprocessing: Brats2020")
            bratfiles.remove('name_mapping.csv')
            bratfiles.remove('survival_info.csv')
            train_images20 = []
            train_masks20 = []
            for file in bratfiles:
                files_path = os.path.join(self.path, file)
                files = os.listdir(os.path.join(self.path, file))
                for i in files:
                    if "flair" in i:
                        train_images20.append(os.path.join(files_path,i))
                    if "seg" in i:
                        train_masks20.append(os.path.join(files_path,i))
            return train_images20,train_masks20

        elif self.dataset == "brats19":
            print("Preprocessing: Brats2019")
            train_images19 = []
            train_masks19 = []
            for file in bratfiles:
                files_path = os.path.join(self.path, file)
                files = os.listdir(os.path.join(self.path, file))
                for i in files:
                    if "flair" in i:
                        train_images19.append(os.path.join(files_path,i))
                    if "seg" in i:
                        train_masks19.append(os.path.join(files_path,i))
            return train_images19,train_masks19
        
        elif self.dataset == "MSD":
            #define paths
            flair_images = "/raid/ai21resch01001/shashank/new_brain/Task01_BrainTumour/imagesTr/"
            flair_masks = "/raid/ai21resch01001/shashank/new_brain/Task01_BrainTumour/labelsTr/"

            #cleaning
            image_files =sorted(os.listdir(flair_images))
            mask_files = sorted(os.listdir(flair_masks))

            image_files = self.clean_files(image_files)
            mask_files = self.clean_files(mask_files)

            image_file_paths = [os.path.join(flair_images,x) for x in image_files]
            mask_file_paths = [os.path.join(flair_masks,x) for x in mask_files]
            return image_file_paths, mask_file_paths

    def read_images_and_masks(self,image_save_path, mask_save_path):
        #scaler
        scaler = MinMaxScaler()
        image_file_paths, mask_file_paths = self.get_path()
        #read images
        for i,(image,mask) in tqdm(enumerate(zip(image_file_paths,mask_file_paths))):
            #read image
            img = nib.load(image).get_fdata()
            img = scaler.fit_transform(img.reshape(-1,img.shape[-1])).reshape(img.shape)
            if self.dataset =="MSD":
              img = img[:,:,:,0]
            #crop
            img = img[56:184,56:184,13:141]
            img = np.expand_dims(img,axis=3)

            #read masks
            mas = nib.load(mask).get_fdata()
            mas = np.where(mas==0, mas, 1)
            mas = mas.astype(np.uint8)
            mas = mas[56:184,56:184,13:141]
            mas = np.expand_dims(mas,axis=3)
            
            np.save(image_save_path + "img{}.npy".format(str(i)),img)
            np.save(mask_save_path + "mask{}.npy".format(str(i)),mas)

#preprocess Brats20
pre = preprocess("/raid/ai21resch01001/shashank/BratsDataset/MICCAI_BraTS2020_TrainingData/",dataset = "brats20" )
pre.read_images_and_masks("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats20/brats20_images/","/raid/ai21resch01001/shashank/new_brain/preprocessed_brats20/brats20_masks/" )

#preprocess Brats19
pre = preprocess("/raid/ai21resch01001/shashank/BratsDataset/HGG",dataset = "brats19" )
pre.read_images_and_masks("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats19/brats19_images/","/raid/ai21resch01001/shashank/new_brain/preprocessed_brats19/brats19_masks/" )

#Get train files path(Brats20)
images_file_brats20 = os.listdir("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats20/brats20_images/")
masks_file_brats20 = os.listdir("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats20/brats20_masks/")
npy_brats20_paths = [os.path.join("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats20/brats20_images/", img_file) for img_file in images_file_brats20[1:]]
npy_brats20_masks = [os.path.join("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats20/brats20_masks/",mas_file) for mas_file in masks_file_brats20[1:]] 

#Get train files path(brats19)
images_file_brats19 = os.listdir("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats19/brats19_images/")
masks_file_brats19 = os.listdir("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats19/brats19_masks/")
npy_brats19_paths = [os.path.join("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats19/brats19_images/", img_file) for img_file in images_file_brats19[1:]]
npy_brats19_masks = [os.path.join("/raid/ai21resch01001/shashank/new_brain/preprocessed_brats19/brats19_masks/", mas_file) for mas_file in masks_file_brats19[1:]]

#Get train files path(MSD)
images_file = os.listdir("/raid/ai21resch01001/shashank/new_brain/preproessed_npy_data_task011/images_128/")
masks_file = os.listdir("/raid/ai21resch01001/shashank/new_brain/preproessed_npy_data_task011/masks_128/")
npy_images_path = [os.path.join("/raid/ai21resch01001/shashank/new_brain/preproessed_npy_data_task011/images_128/", img_file) for img_file in images_file[1:]]
npy_masks_path = [os.path.join("/raid/ai21resch01001/shashank/new_brain/preproessed_npy_data_task011/masks_128/",mas_file) for mas_file in masks_file[1:]]

def load_img(img_list):
    images=[]
    for i, image_name in enumerate(img_list):    
        image = np.load(image_name)
        images.append(image)
    images = np.array(images)
    return(images)

from sklearn.utils import shuffle

npy_images_path, npy_masks_path = shuffle(npy_images_path,npy_masks_path)
npy_brats19_images_paths = shuffle(npy_brats19_paths)
npy_brats20_images_paths = shuffle(npy_brats20_paths)
npy_brats19_mask_paths = shuffle(npy_brats19_masks)
npy_brats20_mask_paths = shuffle(npy_brats20_masks)

test_image = np.load(npy_images_path[1])


'''

#Visualize saved images:
img = np.load(npy_images_path[1])
for i in range(50):
    plt.imshow(img[:,:,i,0],cmap = 'gray')
    plt.show()

'''


"""## 3D-UNet"""

class UNET3DServer(tf.keras.Model):
    def __init__(self,classes):
        super(UNET3DServer,self).__init__()
        self.classes = classes
        # self.encoder_rep = None # s
        
    #helper functions
    def conv3D(self,x,
               filters,
               filter_size,
               activation = 'relu'):
        out = tf.keras.layers.Conv3D(filters,(filter_size,filter_size,filter_size),padding = "same",use_bias=True )(x)
        batch_norm_ = tf.keras.layers.BatchNormalization()(out)
        conv_batch_norm_act = tf.keras.layers.Activation('relu')(batch_norm_)
        return out
    
    def upsampling3D(self,x,
                     filters,
                     filter_size,
                     stride =2,
                     activation = 'relu'):
        up_out = tf.keras.layers.Conv3DTranspose(filters,(filter_size,filter_size,filter_size),strides = (stride,stride,stride),padding = 'same')(x)
        batch_norm_ = tf.keras.layers.BatchNormalization()(up_out)
        return batch_norm_
    
    def concatenate(self,x1,x2):
        concat = tf.keras.layers.concatenate([x1,x2])
        return concat
    
    def max_pool3D(self, x, filter_size,stride, name = None):
        out = tf.keras.layers.MaxPooling3D((filter_size,filter_size,filter_size),strides = stride, name = name)(x)
        return out

    def downconv(self,x,filters, name = None):
        s1 = self.conv3D(x,filters,3)
        s2 = self.conv3D(s1,filters,3)
        return s2

    def upconv(self,x,
               filters,skip_connection):
        e1 = self.upsampling3D(x,filters,2)
        concat = self.concatenate(e1,skip_connection)
        #layer2
        conv1 = self.conv3D(concat,filters,3)
        #layer 3
        conv2 = self.conv3D(conv1,filters,3)
        return conv2

    def call(self):
        #input
        inputs = tf.keras.layers.Input(shape = (128,128,128,1))
        #encoder
        # print(inputs.shape)
        d1 = self.downconv(inputs,32)
        # print(d1.shape)
        m1 = self.max_pool3D(d1,filter_size = 2,stride = 2)
        # print(m1.shape)
        d2 = self.downconv(m1,64)
        m2 = self.max_pool3D(d2,filter_size = 2,stride = 2)
        d3 = self.downconv(m2,128)
        m3 = self.max_pool3D(d3,filter_size = 2,stride = 2)
        d4 = self.downconv(m3,256)
        m4 = self.max_pool3D(d4,filter_size =2,stride=2, name = "layer_before_output")
        # encoder_output = m4
        # print(encoder_output.shape)
        
        #bottleneck
        bridge1 = self.conv3D(m4,1024,3,1)
        bridge2 = self.conv3D(bridge1,1024,3,1)
        # self.encoder_rep = bridge # s

        #decoder
        u1 = self.upconv(bridge2,256,d4)
        u2 = self.upconv(u1,128,d3)
        u3 = self.upconv(u2,64,d2)
        u4 = self.upconv(u3,32,d1)

        #1x1 output
        logits = tf.keras.layers.Conv3D(self.classes,(1,1,1),padding = "same")(u4)
        logits = tf.nn.sigmoid(logits)
        
        #model
        model = tf.keras.Model(inputs = [inputs],outputs = [logits]) 
        
        # print(encoder.summary())
        # print(model.summary())
        
        # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        # client_model = tf.keras.Model(inputs = [inputs], outputs = [m4])
        return model

unet_s = UNET3DServer(1)

class UNET3DClients(tf.keras.Model):
    def __init__(self,classes):
        super(UNET3DClients,self).__init__()
        self.classes = classes
        
    #helper functions
    def conv3D(self,x,
               filters,
               filter_size,
               activation = 'relu',name = None):
        out = tf.keras.layers.Conv3D(filters,(filter_size,filter_size,filter_size),padding = "same",name =name,use_bias = True)(x)
        batch_norm_ = tf.keras.layers.BatchNormalization()(out)
        conv_batch_norm_act = tf.keras.layers.Activation('relu')(batch_norm_)
        return out
    
    def upsampling3D(self,x,
                     filters,
                     filter_size,
                     stride =2,
                     activation = 'relu'):
        up_out = tf.keras.layers.Conv3DTranspose(filters,(filter_size,filter_size,filter_size),strides = (stride,stride,stride),padding = 'same')(x)
        batch_norm_ = tf.keras.layers.BatchNormalization()(up_out)
        return batch_norm_
    
    def concatenate(self,x1,x2):
        concat = tf.keras.layers.concatenate([x1,x2])
        return concat
    
    def max_pool3D(self, x, filter_size,stride, name = None):
        out = tf.keras.layers.MaxPooling3D((filter_size,filter_size,filter_size),strides = stride, name = name)(x)
        return out

    def downconv(self,x,filters,name = None):
        s1 = self.conv3D(x,filters,3, name =name )
        s2 = self.conv3D(s1,filters,3, name = name + "0")
        return s2

    def call(self):
        #input
        inputse = tf.keras.layers.Input(shape = (128,128,128,1))
        #encoder
        d1e = self.downconv(inputse,32, name = "layer1")
        m1e = self.max_pool3D(d1e,filter_size = 2,stride = 2)
        d2e = self.downconv(m1e,64,  name = "layer2")
        m2e = self.max_pool3D(d2e,filter_size = 2,stride = 2)
        d3e = self.downconv(m2e,128, name = "layer3")
        m3e = self.max_pool3D(d3e,filter_size = 2,stride = 2)
        d4e = self.downconv(m3e,256, name = "layer4")
        m4e = self.max_pool3D(d4e,filter_size =2,stride=2)

        #model
        modele = tf.keras.Model(inputs = [inputse],outputs = [m4e]) 
        return modele
client_unet = UNET3DClients(1)

#dice Coeff
def dice_coef(y_true, y_pred, smooth=1.0):
    
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

# Computing Precision
def precision(y_true, y_pred):
    
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    
    return precision

# Computing Sensitivity
def sensitivity(y_true, y_pred):
    
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    
    return true_positives / (possible_positives + K.epsilon())

# Computing Specificity
def specificity(y_true, y_pred):
    
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    
    return true_negatives / (possible_negatives + K.epsilon())

def mini_batches_( X, Y, batch_size=64):
    """
    function to produce minibatches for training
    :param X: input placeholder
    :param Y: mask placeholder
    :param batch_size: size of each batch
    :return:
    minibatches for training
    """
    train_length = len(X)
    num_batches = int(np.floor(train_length / batch_size))
    batches = []
    for i in tqdm(range(num_batches)):
        batch_x = X[i * batch_size: i * batch_size + batch_size]
        batch_y = Y[i * batch_size:i * batch_size + batch_size]
        batch_x = load_img(batch_x)
        batch_y = load_img(batch_y)
        batch_x = batch_x.astype(np.float32)
        batch_y = batch_y.astype(np.float32)
        batches.append([batch_x, batch_y])
    return batches
    
def mini_batches_clients( X, batch_size=64):
    """
    function to produce minibatches for training
    :param X: input placeholder
    :param Y: mask placeholder
    :param batch_size: size of each batch
    :return:
    minibatches for training
    """
    train_length = len(X)
    num_batches = int(np.floor(train_length / batch_size))
    batches = []
    for i in tqdm(range(num_batches)):
        batch_x = X[i * batch_size: i * batch_size + batch_size]
        batch_x = load_img(batch_x)
        batch_x = batch_x.astype(np.float32)
        batches.append([batch_x])
    return batches

#load clients and data
metrics = [sm.metrics.IOUScore(threshold=0.5),dice_coef,precision,sensitivity,specificity]

LR = 0.0001
optim = tf.keras.optimizers.Adam(LR)

#Client 1
client_1 ={}
client_1["original_data"] = mini_batches_(npy_brats20_images_paths,npy_brats20_mask_paths,2)
client_1["model"] = client_unet.call()
client_1["optimizer"] =tf.keras.optimizers.Adam(LR)

#Client 2
client_2 ={}
client_2["original_data"] = mini_batches_(npy_brats19_images_paths,npy_brats19_mask_paths,2)
client_2["model"] = client_unet.call()
client_2["optimizer"] =tf.keras.optimizers.Adam(LR)

#Server 
Server=dict()
Server["model"]= unet_s.call()
Server["original_data"] = mini_batches_(npy_images_path,npy_masks_path,2)
Server["optimizer"] = tf.keras.optimizers.Adam(LR)

client_2["model"] = client_unet.call()
client_1["model"] = client_unet.call()
Server["model"] = unet_s.call()

#Losses
dice_loss = sm.losses.DiceLoss() 
focal_loss = sm.losses.BinaryFocalLoss()
total_loss = dice_loss + (1 * focal_loss)



'''
del npy_images_path
del npy_masks_path
del npy_brats20_images_paths
del npy_brats19_images_paths
'''




"""#Clients Training loop"""

test_data_gen = mini_batches_(npy_images_path[400:420], npy_masks_path[400:420],2)

def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    #get the average grad accross all client gradients
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_mean(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
    return avg_grad

from matplotlib.pylab import *

def set_client_weights(server,client):
    #get server weights
    server_weights = []
    for slayer in server.layers:
        server_weights.append(server.get_layer(slayer.name).get_weights())
        if slayer.name == "layer_before_output":
            break
    
    #set client weights:
    print(nan in server_weights)
    for i, clayer in enumerate(client.layers):
        clayer.set_weights(server_weights[i])
    return client
    
def set_server_weights(server,client):
    client_weights = []
    for clayer in client.layers:
        client_weights.append(client.get_layer(clayer.name).get_weights())
    #set client weights:
    for i, slayer in enumerate(server.layers):
        slayer.set_weights(client_weights[i])
        if slayer.name == "layer_before_output":
            break
    return server

import gc
gc.collect()

client_2["model"] = client_unet.call()
client_1["model"] = client_unet.call()
Server["model"] = unet_s.call()

# def contrastive_loss(z_prev,z_present,margin = 0.2):
#     dis = tf.sqrt(tf.reduce_sum(tf.square(z_present - z_prev)) + 1.0e-12)
#     # dis = tf.norm(z_present - z_prev, ord = "euclidean",axis = -1)
#     zeros = np.zeros(dis.shape)
#     loss = tf.math.square(tf.math.maximum(zeros, margin - dis))
#     loss = tf.math.reduce_mean(loss)
#     return loss

def contrastive_loss(prev, pres, serv, tau=1):
    cosine_similarity = tf.keras.losses.CosineSimilarity(axis=1)
    # cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))
    loss_con = - tf.math.log(tf.math.exp(cosine_similarity(pres, serv) / tau) / (tf.math.exp(cosine_similarity(pres, serv) / tau) + tf.math.exp(cosine_similarity(pres, prev) / tau)))
    return loss_con

#define clients(test)
batch_size=2
steps_per_epoch = 200//batch_size
val_steps_per_epoch = 84//batch_size
clients = [client_1, client_2]
iou = sm.metrics.IOUScore(threshold=0.5)
epochs = 1
batch_size= 2
past_model = None
temperature = 0.5
mu = 1
num_comm = 1

dice_loss = sm.losses.DiceLoss() 
focal_loss = sm.losses.BinaryFocalLoss()
total_loss = dice_loss + (1 * focal_loss)

for t in range(num_comm):
    print('\ncommunication round: ',t+1,'\n')
    i = 1
    local_weight_list = list()
    for client in clients:
        print(f'Training Client {i}')
        i += 1
        
        # check1 = curr
        optimizer = client['optimizer']
        for epoch in range(epochs):
          print('\nEpoch ', epoch+1)
          for batch_idx, (image, target) in enumerate(tqdm(client['original_data'][:4])):
              # print(batch_idx, len(image))
              curr = client['model']
              server_model = Server['model']
              
              enc_input = tf.keras.layers.Input(shape = (128,128,128,1))
              enc = server_model.layers[0](enc_input)
              for layer in server_model.layers[1:12]:
                  enc = layer(enc)
              enc_output = server_model.layers[12](enc)
              encoder = tf.keras.Model(inputs = enc_input, outputs = enc_output)
              serv = encoder(image)
              
              # print(temp_output.shape)
              # serv = server_model(image)
              # print('\nmodel output shape: ',temp.shape,'\nserv variable shape: ',serv.shape)
              # serv = server_model.encoder_rep # s
              
              with tf.GradientTape() as tape:
              
                  output = curr(image)
                  
                  if past_model == None:
                      past = tf.zeros_like(output)
                  else:
                      past = past_model(image)
                      
                  # print('\nIs current equal to past? ',(curr.numpy==past.numpy).all())
                  #print('\nPast representation shape: ',past.shape)
                  #print('\nPresent representation shape: ',output.shape)
                  #print('\nGlobal representation shape: ', serv.shape)
                  
                  cl = contrastive_loss(past,output,serv)
                  print(cl)
                  loss = mu * cl
                  past_model = curr
              grads = tape.gradient(loss, curr.trainable_weights)
              optimizer.apply_gradients(zip(grads, curr.trainable_variables))
              # check2 = curr
              # print(np.allclose(check1, check2))
              client['model'] = curr
        local_weight_list.append(client['model'].get_weights())

    # train server
    print('\nTraining server\n')
    server_model = Server['model']
    CE_loss = tf.keras.losses.CategoricalCrossentropy() 
    optimizer = Server['optimizer']
    
    average_weights = sum_scaled_weights(local_weight_list)
    
        
    client_model = clients[0]['model']
    client_model.set_weights(average_weights)
    client_layer_weights = []
    
    for clayer in client_model.layers:
       client_layer_weights.append(client_model.get_layer(clayer.name).get_weights())
    num_client_layer = len(client_layer_weights)
    
    
#    for i, slayer in enumerate(server_model.layers[1:12]):
#     slayer.set_weights(client_layer_weights[i])
        
    
    for i, slayer in enumerate(server_model.layers):
      if i == num_client_layer:
        break
      slayer.set_weights(client_layer_weights[i])
    
    
    
    for epoch in range(epochs):
      server_loss = []
      for batch_idx, (img, target) in enumerate(tqdm(Server['original_data'][:5])):
          img = img[:,:,:,0]
          # img = img[56:184,56:184,13:141]
          # img = np.expand_dims(img,axis=3)
          
          with tf.GradientTape() as tape:
              output = server_model(img)
              s_loss = total_loss(target, output)
              server_loss.append(s_loss.numpy())
              
              # test_image = img
              im = output
              test_target = target
          grads = tape.gradient(s_loss, server_model.trainable_weights)
          optimizer.apply_gradients(zip(grads, server_model.trainable_variables))
              # print('\nImage shape: ', im.shape, '\nTarget shape: ', target.shape)
      server_loss = np.array(server_loss).mean()
      print('\nServer loss: ', server_loss,'\n')
    Server['model'] = server_model
    
    server_weights = []
    
    for i, slayer in enumerate(Server['model'].layers):
      if i == num_client_layer:
        break
        
      server_weights.append(Server['model'].get_layer(slayer.name).get_weights())
    
    
    for client in  clients:
      for i, clayer in enumerate(client['model'].layers):
        clayer.set_weights(server_weights[i])
    
    # im = server_model(test_image)
    
    plt.imshow(im[0,:,:,1,0],cmap = 'gray')
    plt.savefig('img.jpg')
    plt.imshow(test_target[0,:,:,1,0],cmap='gray')
    plt.savefig('target.jpg')



# server_model = Server["model"]
# average_weights = sum_scaled_weights(local_weight_list)

# average_weights


CE_loss = tf.keras.losses.CategoricalCrossentropy() 

dice = []
pre = []
batch_loss=[]
se=[]
spe = []
io = []

model = Server["model"]

for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
    logits = model(images)
    loss = total_loss(masks,logits)
    batch_loss.append(loss)
    #metrics
    dice.append(dice_coef(masks,logits))
    pre.append(precision(masks,logits))
    se.append(sensitivity(masks,logits))
    spe.append(specificity(masks,logits))
    io.append(iou(masks,logits))        
batch_loss = np.array(batch_loss).mean()
dice = np.array(dice).mean()
pre =np.array(pre).mean()
se =  np.array(se).mean()
spe =  np.array(spe).mean()
io =  np.array(io).mean()

print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))


'''
dice = []
pre = []
batch_loss=[]
se=[]
spe = []
io = []


i = 0
for client in clients:
  i += 1
  model = client['model']

  for batch_idx, (images, masks) in enumerate(tqdm(client['original_data'][4:5])):
      logits = model(images)
      loss = CE_loss(masks,logits)
      batch_loss.append(loss)
      #metrics
      dice.append(dice_coef(masks,logits))
      pre.append(precision(masks,logits))
      se.append(sensitivity(masks,logits))
      spe.append(specificity(masks,logits))
      io.append(iou(masks,logits))        


  batch_loss = np.array(batch_loss).mean()
  dice = np.array(dice).mean()
  pre =np.array(pre).mean()
  se =  np.array(se).mean()
  spe =  np.array(spe).mean()
  io =  np.array(io).mean()
  print('\nClient ',i,'\n')
  print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))

'''


model.save("/raid/ai21resch01001/shashank/saved_weights/" + "weights_epoch100_final.h5")


model = Server["model"]
num = 0
path = "/raid/ai21resch01001/shashank/results/"
for batch_idx, (images, masks) in enumerate(test_data_gen):   
    
    test_img = images
    test_mask = masks
    # test_img_input = np.expand_dims(test_img, axis=0)
    test_prediction = model.predict(test_img)
    # test_prediction = test_prediction[0,:,:,:,0]
    # test_prediction = np.argmax(test_prediction,axis = 2)

    num += 1
    
    # print(test_prediction_argmax.shape)
    # print(test_mask_argmax.shape)
    # print(np.unique(test_prediction_argmax))
    #Plot individual slices from test predictions for verification
    for o in range(100):
     #n_slice=random.randint(0, test_prediction_argmax.shape[2])
        #plt.figure(figsize=(12, 8))
        #plt.subplot(231)
        #plt.title('Testing Image')
        plt.imshow(test_img[:,:,o], cmap = 'gray')
        fname = path + 'img_' + str(num) + '_' + str(o)
        plt.savefig(fname)
        # plt.subplot(232)
        # plt.title('Testing Label')
        plt.imshow(test_mask[:,:,o], cmap = 'gray')
        fname = path + 'mask_' + str(num) + '_' + str(o)
        plt.savefig(fname)
        #plt.subplot(233)
        #plt.title('Prediction on test image')
        plt.imshow(test_prediction[:,:,o], cmap = 'gray')
        fname = path + 'pred_' + str(num) + '_' + str(o)
        plt.savefig(fname)


print('DONE!!!')



'''

#calculate z_glob,z,z_prev
for epoch in range(epochs):
    print('\nEpoch:',epoch+1,'\n')
    i = 1
    local_weight_list = list()
    for client in clients:
        print(f'Training Client {i}')
        client_model = client['model']
        optimizer = client["optimizer"]
        #save client state
        client_prev = client_model
        #set client weights to global:
        #update client weight model weights:
        #client_weights_model = set_client_weights(Server["model"], client_weights_model)
        #present_client_model = client_weights_model
        #present_client_model = client_model.set_weights(client_weights_model.get_weights())
        print("Checking weights after the Server weights update..\n\n")
        present_client_model = set_client_weights(Server["model"],client_model)
        #set server weights
        average_weights = []
        con_loss = []
        #Don't Run (For future purposes)
        for batch_idx, images in enumerate(tqdm(client["original_data"][:5])):
            server_model = Server["model"]
            local_client_weight = []
            #train
            with tf.GradientTape() as tape:
                #Constractive learning loss starts
                z_t_1 = client_prev(images)
                z_t = present_client_model(images)
                #print(present_client_model.trainable_variables) #s
                #logits,z_glob= server_model(images)
                #print("Checking for Nan")
                # print(z_t_1) #s
                loss = contrastive_loss(z_t_1,z_t)
                print(loss) #s
                #print(present_client_model.get_weights())
                con_loss.append(loss)
                #print(present_client_model.get_weights())
                #loss -> Contrastive Loss
                #cos = tf.keras.losses.CosineSimilarity()
                #pos1 =  cos(z_t,z_glob)
                #nega = cos(z_t,z_t_1)
                #logits = tf.reshape(pos1,(-1,1))
                #nega = tf.reshape(nega,(-1,1))
                #concat
                #logits = tf.concat([logits,nega],axis = -1)
                #logits/=temperature
                #labels = tf.zeros(len(images))
                #cross-entropy
                #loss = contrastive_loss(labels,logits)
                #cross = tf.keras.losses.BinaryCrossentropy()
                #loss = 0.001 * cross(logits,labels)
                #constractive loss implementation end
            grads = tape.gradient(loss, present_client_model.trainable_weights)
            #print(grads) #s
            optimizer.apply_gradients(zip(grads, present_client_model.trainable_variables))
            #print(present_client_model.trainable_variables) #s
            # print(z_t_1) #s
            
            #local_weights
            #get weights
        local_weight_list.append(present_client_model.get_weights())
        # run this to check client weights after loss calculation and gradient updation. It will produce NAN values.
        #print(local_weight_list) 
        con_loss = np.array(con_loss).mean()
        print("Epoch: {} | Contrastive Loss: {}".format(epoch, con_loss))
    #server wieghts:
    model = Server["model"]
    #local_weight_list        
    print("UPDATING SERVER...\n\n")
    average_weights = sum_scaled_weights(local_weight_list)
    client_weights_model.set_weights(average_weights)
    #update server
    model = set_server_weights(model, client_weights_model)
    print("SERVER UPDATED...\n\n")
    pre = []
    batch_loss=[]
    se=[]
    spe = []
    io = []
    dice = []
    server_model = Server["model"]
    for batch_idx, (images,masks) in enumerate(tqdm(Server["original_data"][:5])):
        with tf.GradientTape() as tape:
            logits = server_model(images)
            sloss = total_loss(masks,logits)
            batch_loss.append(sloss)
            dice.append(dice_coef(masks,logits))
            pre.append(precision(masks,logits))
            se.append(sensitivity(masks,logits))
            spe.append(specificity(masks,logits))
            io.append(iou(masks,logits))
        grads = tape.gradient(sloss, server_model.trainable_variables)
        optimizer.apply_gradients(zip(grads, server_model.trainable_variables))
    batch_loss = np.array(batch_loss).mean()
    dice = np.array(dice).mean()
    pre =np.array(pre).mean()
    se =  np.array(se).mean()
    spe =  np.array(spe).mean()
    io =  np.array(io).mean()
    i+=1
    print("Loss: {} | Dice Coeff: {}  |\n\n Precision: {} Sensitivity: {} | \n\n Specificity: {} , IOU: {} | \n\n".format(batch_loss,dice,pre,se,spe,io))

            

    if epoch % 5 ==0 and epoch != 0 :
        print("\n\n TESTING SERVER...\n")
        dice = []
        pre = []
        batch_loss=[]
        se=[]
        spe = []
        io = []
        model = Server["model"]
        for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
            logits = model(images)
            loss = total_loss(masks,logits)
            batch_loss.append(loss)
            #metrics
            dice.append(dice_coef(masks,logits))
            pre.append(precision(masks,logits))
            se.append(sensitivity(masks,logits))
            spe.append(specificity(masks,logits))
            io.append(iou(masks,logits))        
        batch_loss = np.array(batch_loss).mean()
        dice = np.array(dice).mean()
        pre =np.array(pre).mean()
        se =  np.array(se).mean()
        spe =  np.array(spe).mean()
        io =  np.array(io).mean()
        print("\nServer Epoch: {} , Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(epoch+1,batch_loss,dice,pre,se,spe,io))
    
'''

"""## New"""

'''

def set_server_weights(server,weights):
    """client_weights = []
    for clayer in client.layers:
        client_weights.append(client.get_layer(clayer.name).get_weights())"""
    #set client weights:
    for i, slayer in enumerate(server.layers[1:]):
        slayer.set_weights(weights[i])
        print(weights[i].shape)
        print(slayer.shape)
        if slayer.name == "layer_before_output":
            break
    return server

#define clients(test)
batch_size=2
steps_per_epoch = 200//batch_size
val_steps_per_epoch = 84//batch_size
clients = [client_1, client_2]
iou = sm.metrics.IOUScore(threshold=0.5)
epochs = 50
batch_size= 2
temperature = 0.5


#calculate z_glob,z,z_prev
for epoch in range(epochs):
    print('\nEpoch:',epoch+1,'\n')
    i = 1
    local_weight_list = list()
    for client in clients:
        print(f'Training Client {i}')
        client_model = client['model']
        optimizer = client["optimizer"]
        #save client state
        client_prev = client_model
        #set client weights to global:
        #update client weight model weights:
        #client_weights_model = set_client_weights(Server["model"], client_weights_model)
        #present_client_model = client_weights_model
        #present_client_model = client_model.set_weights(client_weights_model.get_weights())
        present_client_model = set_client_weights(Server["model"],client_model)
        #set server weights
        average_weights = []
        #Don't Run (For future purposes)
        for batch_idx, images in enumerate(tqdm(client["original_data"][:5])):
            server_model = Server["model"]
            local_client_weight = []
            #train
            with tf.GradientTape() as tape:
                #Constractive learning loss starts
                z_t_1 = client_prev(images)
                z_t = present_client_model(images)
                #logits,z_glob= server_model(images)

                loss = contrastive_loss(z_t_1,z_t)
                
                #loss -> Contrastive Loss
                #cos = tf.keras.losses.CosineSimilarity()
                #pos1 =  cos(z_t,z_glob)
                #nega = cos(z_t,z_t_1)
                #logits = tf.reshape(pos1,(-1,1))
                #nega = tf.reshape(nega,(-1,1))
                #concat
                #logits = tf.concat([logits,nega],axis = -1)
                #logits/=temperature
                #labels = tf.zeros(len(images))
                #cross-entropy
                #loss = contrastive_loss(labels,logits)
                #cross = tf.keras.losses.BinaryCrossentropy()
                #loss = 0.001 * cross(logits,labels)
                #constractive loss implementation end
            grads = tape.gradient(loss, present_client_model.trainable_weights)
            optimizer.apply_gradients(zip(grads, present_client_model.trainable_variables))
            #local_weights
            #get weights
        local_weight_list.append(present_client_model.get_weights())

        print("Epoch: {} ".format(epoch))
    #server wieghts:
    model = Server["model"]
    #local_weight_list        
    print("UPDATING SERVER...\n\n")
    average_weights = sum_scaled_weights(local_weight_list)
    print(average_weights)
    client_weights_model.set_weights(average_weights)
    #update server
    model = set_server_weights(model, average_weights)
    print("SERVER UPDATED...\n\n")
    pre = []
    batch_loss=[]
    se=[]
    spe = []
    io = []
    dice = []
    server_model = Server["model"]
    for batch_idx, (images,masks) in enumerate(tqdm(Server["original_data"])):
        with tf.GradientTape() as tape:
            logits = server_model(images)
            sloss = total_loss(masks,logits)
            batch_loss.append(sloss)
            dice.append(dice_coef(masks,logits))
            pre.append(precision(masks,logits))
            se.append(sensitivity(masks,logits))
            spe.append(specificity(masks,logits))
            io.append(iou(masks,logits))
        grads = tape.gradient(sloss, server_model.trainable_variables)
        optimizer.apply_gradients(zip(grads, server_model.trainable_variables))

    batch_loss = np.array(batch_loss).mean()
    dice = np.array(dice).mean()
    pre =np.array(pre).mean()
    se =  np.array(se).mean()
    spe =  np.array(spe).mean()
    io =  np.array(io).mean()
    i+=1
    print("Loss: {} | Dice Coeff: {}  |\n\n Precision: {} Sensitivity: {} | \n\n Specificity: {} , IOU: {} | \n\n".format(batch_loss,dice,pre,se,spe,io))

            

    if epoch % 5 ==0 and epoch != 0 :
        print("\n\n TESTING SERVER...\n")
        dice = []
        pre = []
        batch_loss=[]
        se=[]
        spe = []
        io = []
        model = Server["model"]
        for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
            logits = model(images)
            loss = total_loss(masks,logits)
            batch_loss.append(loss)
            #metrics
            dice.append(dice_coef(masks,logits))
            pre.append(precision(masks,logits))
            se.append(sensitivity(masks,logits))
            spe.append(specificity(masks,logits))
            io.append(iou(masks,logits))        
        batch_loss = np.array(batch_loss).mean()
        dice = np.array(dice).mean()
        pre =np.array(pre).mean()
        se =  np.array(se).mean()
        spe =  np.array(spe).mean()
        io =  np.array(io).mean()
        print("\nServer Epoch: {} , Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(epoch+1,batch_loss,dice,pre,se,spe,io))

"""## Original"""

#define clients(test)
batch_size=2
steps_per_epoch = 200//batch_size
val_steps_per_epoch = 84//batch_size
clients = [client_1, client_2]
iou = sm.metrics.IOUScore(threshold=0.5)
epochs = 50
batch_size= 2
temperature = 0.5
"""plot_total_loss = []
plot_dice=[]
plot_pre = []
plot_se=[]
plot_spe = []
plot_io = []"""
#calculate z_glob,z,z_prev
for epoch in range(epochs):
    print('\nEpoch:',e+1,'\n')
    i = 1
    for client in clients:
        print(f'Training Client {i}')
        client_model = client['model']
        optimizer = client["optimizer"]
        #save client state
        client_prev = client_model
        #set client weights to global:
        present_client_model = set_client_weights(Server["model"],client_model)
        #set server weights
        
        local_weight_list = list()
        average_weights = []
        
        #Don't Run (For future purposes)
        for batch_idx, images in enumerate(tqdm(client["original_data"][:5])):
            server_model = Server["model"]
            #train
            with tf.GradientTape() as tape:
                z_t_1 = client_prev(images)
                z_t = present_client_model(images)
                logits,z_glob= server_model(images)
                
                #loss -> Contrastive Loss
                cos = tf.keras.losses.CosineSimilarity()
                pos1 =  cos(z_t,z_glob)
                nega = cos(z_t,z_t_1)
                logits = tf.reshape(pos1,(-1,1))
                nega = tf.reshape(nega,(-1,1))
                #concat
                logits = tf.concat([logits,nega],axis = -1)
                logits/=temperature
                labels = tf.zeros(len(images))
                #cross-entropy
                cross = tf.keras.losses.BinaryCrossentropy()
                loss = 0.001 * cross(logits,labels)
            grads = tape.gradient(loss, present_client_model.trainable_variables)
            optimizer.apply_gradients(zip(grads, present_client_model.trainable_variables))
            #local_weights
            #get weights
            for clayer in present_client_model.layers:
                local_weight_list.append(present_client_model.get_layer(clayer.name).get_weights())
                if clayer.name == "layer_before_output":
                    break
    print(len(local_weight_list))
    #update server
    print("UPDATING SERVER...\n\n")
    server_model = Server["model"]
    average_weights = sum_scaled_weights(local_weight_list)
    print(len(average_weights))
    for o,slayer in enumerate(server_model.layers[0:13]):
        slayer.set_weights(average_weights[o])
        
    print("SERVER UPDATED...\n\n")
    pre = []
    batch_loss=[]
    se=[]
    spe = []
    io = []
    dice = []
    for batch_idx, images,masks in enumerate(tqdm(Server["original_data"])):
        with tf.GradientTape() as tape:
            logits = server_model(images)
            sloss = total_loss(masks,logits)
            batch_loss.append(sloss)

            dice.append(dice_coef(masks,logits))
            pre.append(precision(masks,logits))
            se.append(sensitivity(masks,logits))
            spe.append(specificity(masks,logits))
            io.append(iou(masks,logits))
        grads = tape.gradient(sloss, server_model.trainable_variables)
        optimizer.apply_gradients(zip(grads, server_model.trainable_variables))

    batch_loss = np.array(batch_loss).mean()
    dice = np.array(dice).mean()
    pre =np.array(pre).mean()
    se =  np.array(se).mean()
    spe =  np.array(spe).mean()
    io =  np.array(io).mean()
    i+=1
    print("Loss: {} | Dice Coeff: {}  |\n\n Precision: {} Sensitivity: {} | \n\n Specificity: {} , IOU: {} | \n\n".format(batch_loss,dice,pre,se,spe,io))

            

    if epoch % 5 ==0 and epoch != 0 :
        print("\n\n TESTING SERVER...\n")
        dice = []
        pre = []
        batch_loss=[]
        se=[]
        spe = []
        io = []
        model = Server["model"]
        for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
            logits = model(images)
            loss = total_loss(masks,logits)
            batch_loss.append(loss)
            #metrics
            dice.append(dice_coef(masks,logits))
            pre.append(precision(masks,logits))
            se.append(sensitivity(masks,logits))
            spe.append(specificity(masks,logits))
            io.append(iou(masks,logits))        
        batch_loss = np.array(batch_loss).mean()
        dice = np.array(dice).mean()
        pre =np.array(pre).mean()
        se =  np.array(se).mean()
        spe =  np.array(spe).mean()
        io =  np.array(io).mean()
        print("\nServer Epoch: {} , Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(epoch+1,batch_loss,dice,pre,se,spe,io))

"""## Test"""

server_model = Server["model"]
average_weights = sum_scaled_weights(local_weight_list)

average_weights

dice = []
pre = []
batch_loss=[]
se=[]
spe = []
io = []
model = Server["model"]
for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
    logits = model(images)
    loss = total_loss(masks,logits)
    batch_loss.append(loss)
    #metrics
    dice.append(dice_coef(masks,logits))
    pre.append(precision(masks,logits))
    se.append(sensitivity(masks,logits))
    spe.append(specificity(masks,logits))
    io.append(iou(masks,logits))        
batch_loss = np.array(batch_loss).mean()
dice = np.array(dice).mean()
pre =np.array(pre).mean()
se =  np.array(se).mean()
spe =  np.array(spe).mean()
io =  np.array(io).mean()
print("Epoch: {} , Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(epoch+1,batch_loss,dice,pre,se,spe,io))

model.save("/content/saved_weights/" + "weights_epoch100.h5")

"""## Plot Prediction and Ground Truth"""

model = Server["model"]
for batch_idx, (images, masks) in enumerate(test_data_gen):   
    test_img = images[0,:,:,:,0]
    test_mask = masks[0,:,:,:,0]

    test_img_input = np.expand_dims(test_img, axis=0)
    test_prediction = model.predict(test_img_input)
    test_prediction = test_prediction[0,:,:,:,0]
    #test_prediction = np.argmax(test_prediction,axis = 2)

    
    

    # print(test_prediction_argmax.shape)
    # print(test_mask_argmax.shape)
    # print(np.unique(test_prediction_argmax))
    #Plot individual slices from test predictions for verification
    for o in range(100):
     #n_slice=random.randint(0, test_prediction_argmax.shape[2])
        plt.figure(figsize=(12, 8))
        plt.subplot(231)
        plt.title('Testing Image')
        plt.imshow(test_img[:,:,o], cmap='gray')
        plt.subplot(232)
        plt.title('Testing Label')
        plt.imshow(test_mask[:,:,o])
        plt.subplot(233)
        plt.title('Prediction on test image')
        plt.imshow(test_prediction[:,:,o])
        plt.show()

# !zip -r results_withGAN150.zip results/


'''


